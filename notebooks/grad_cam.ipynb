{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1e423-bfe9-4d46-bd7c-b1dedcf77aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from dfdetect.config import Paths, CLA\n",
    "from dfdetect.data_loaders import (\n",
    "    DFDC_preprocessed_single_frames,\n",
    "    Oversampled,\n",
    "    DFDC,\n",
    "    CelebDFV2_preprocessed,\n",
    "    CelebDFV2,\n",
    ")\n",
    "from dfdetect.models.dfdetect_model import (\n",
    "    FeatureType,\n",
    "    SteganalysisModel,\n",
    "    TimmModel,\n",
    "    TNTPl,\n",
    "    SrnetPl,\n",
    "    SRNetDouble,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "from dfdetect.utils import CropResize, Slurm\n",
    "import dfdetect.utils as utils\n",
    "\n",
    "from torchvision import transforms\n",
    "from dfdetect.data_augmentation import DataAugmentations\n",
    "from copy import copy\n",
    "\n",
    "import pytorch_grad_cam as gc\n",
    "import matplotlib.pyplot as plt\n",
    "from dfdetect.data_loaders import Oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182ea80-4ff7-4415-99ec-284e137c1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fb9de-2265-43a8-98fa-592c2e4630ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d65dd-300c-4084-a7f1-c9cf8023ec63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d70ddb-6398-4c15-8c45-80eaff0c045f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_args():\n",
    "    \"\"\"Setup all the command line arguments supported in this script.\"\"\"\n",
    "    CLA.register(\"project_name\", \"DFDC-full-single-images\")\n",
    "    CLA.register(\"dataset\", \"DFDC\", choices=[\"DFDC\", \"celebdfv2\"])\n",
    "    CLA.register(\"seed\", 0x1B)\n",
    "    CLA.register(\"test\", False)\n",
    "    CLA.register(\"valid\", False)\n",
    "    CLA.register(\"test_all_frames\", False)\n",
    "    CLA.register(\"debug\", False)\n",
    "    CLA.register(\"model\", \"stegano\")\n",
    "    CLA.register(\"stegano_spatial_model\", \"legacy_seresnet18\")\n",
    "    CLA.register(\"target_image_size\", 128)\n",
    "    CLA.register(\"spatial_features\", True)\n",
    "    CLA.register(\"spectral_features\", True)\n",
    "    CLA.register(\"dct_features\", True)\n",
    "    CLA.register(\"rgb_to_ycc\", True)\n",
    "    CLA.register(\"rgb_to_gray\", False)\n",
    "    CLA.register(\"batch_size\", 64)\n",
    "    CLA.register(\"accumulate_grad_batches\", 1)\n",
    "    CLA.register(\"decision_threshold\", 0.5)  # When predicting accuracy and f1 score\n",
    "    CLA.register(\"nb_epochs\", 200)\n",
    "    CLA.register(\"act_function\", \"relu\", choices=[\"relu\", \"gelu\"])\n",
    "    CLA.register(\"srnet_double_nb_features\", 512)\n",
    "    CLA.register(\"srnet_double_num_type2_layers\", 5)\n",
    "    CLA.register(\"srnet_double_type_3_layer_sizes\", [16, 64, 128, 256])\n",
    "    CLA.register(\"srnet_double_type_2_layer_feat_size\", 16)\n",
    "    CLA.register(\"srnet_double_type_1_kernel_size_spatial\", 3)\n",
    "    CLA.register(\"srnet_double_type_1_kernel_size_spectral\", 2)\n",
    "    CLA.register(\"srnet_double_with_attention\", False)\n",
    "    CLA.register(\"plot_ttest\", False)\n",
    "    CLA.register(\"confidence_to_stop\", 0.05)\n",
    "\n",
    "    CLA.parse()\n",
    "\n",
    "\n",
    "def get_transforms(set_type):\n",
    "    \"\"\"Get transforms for a single RGB frame, only set_type=train will add probabilistic data augmentations\"\"\"\n",
    "    # Means and variance computed from the training set with the function\n",
    "    # utils.compute_running_stats(train_set)\n",
    "\n",
    "    assert not (\n",
    "        CLA.rgb_to_ycc & CLA.rgb_to_gray\n",
    "    ), \"Only one of rgb_to_ycc and rgb_to_gray can be True\"\n",
    "    if CLA.rgb_to_ycc:\n",
    "        means = torch.tensor([0.3443, 0.5621, 0.4715])\n",
    "        stds = torch.tensor([0.0377, 0.0017, 0.0010]).sqrt_()\n",
    "    elif CLA.rgb_to_gray:\n",
    "        means = torch.tensor([0.5])\n",
    "        stds = torch.tensor([0.0833]).sqrt_()\n",
    "    else:\n",
    "        means = torch.tensor([0.485, 0.456, 0.406])\n",
    "        stds = torch.tensor([0.229, 0.224, 0.225])\n",
    "    all_transforms = [\n",
    "        CropResize(CLA.target_image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=means, std=stds),\n",
    "    ]\n",
    "    if CLA.rgb_to_ycc:\n",
    "        all_transforms.insert(0, utils.rgb_to_ycc)\n",
    "    elif CLA.rgb_to_gray:\n",
    "        all_transforms.insert(0, utils.rgb_to_gray)\n",
    "    if set_type == \"train\":  # Add data augmentation for train set in RGB domain\n",
    "        all_transforms.insert(0, DataAugmentations())\n",
    "\n",
    "    return transforms.Compose(all_transforms)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model_name = CLA.model.lower()\n",
    "    features = 0\n",
    "    if CLA.dct_features:\n",
    "        features |= FeatureType.DCT\n",
    "    if CLA.spatial_features:\n",
    "        features |= FeatureType.YCC\n",
    "\n",
    "    if model_name == \"stegano\":\n",
    "        model_class = SteganalysisModel\n",
    "        model_args = dict(\n",
    "            features=features,\n",
    "            original_stegano=True,\n",
    "            spatial_model=CLA.stegano_spatial_model,\n",
    "        )\n",
    "    elif model_name == \"tnt\":\n",
    "        assert CLA.target_image_size == 224\n",
    "        model_class = TNTPl\n",
    "        model_args = dict(im_size=CLA.target_image_size)\n",
    "    elif model_name == \"seresnet\":\n",
    "        model_class = SteganalysisModel\n",
    "        model_args = dict(\n",
    "            features=features,\n",
    "            original_stegano=False,\n",
    "            spatial_model=CLA.stegano_spatial_model,\n",
    "        )\n",
    "    elif model_name == \"srnet\":\n",
    "        model_class = SrnetPl\n",
    "        model_args = dict(in_channels=1 if CLA.rgb_to_gray else 3)\n",
    "    elif model_name == \"srnetdouble\":\n",
    "        model_class = SRNetDouble\n",
    "        model_args = dict(\n",
    "            features=features,\n",
    "            srnet_args_spatial={\n",
    "                \"act_function\": F.gelu if CLA.act_function == \"gelu\" else F.relu,\n",
    "                \"nb_features\": CLA.srnet_double_nb_features,\n",
    "                \"type_3_layer_sizes\": [\n",
    "                    int(tmp) for tmp in CLA.srnet_double_type_3_layer_sizes\n",
    "                ],\n",
    "                \"num_type2_layers\": CLA.srnet_double_num_type2_layers,\n",
    "                \"type_2_layer_feat_size\": CLA.srnet_double_type_2_layer_feat_size,\n",
    "                \"type_1_kernel_size\": CLA.srnet_double_type_1_kernel_size_spatial,\n",
    "            },\n",
    "            srnet_args_spectral={\n",
    "                \"act_function\": F.gelu if CLA.act_function == \"gelu\" else F.relu,\n",
    "                \"nb_features\": CLA.srnet_double_nb_features,\n",
    "                \"type_3_layer_sizes\": [\n",
    "                    int(tmp) for tmp in CLA.srnet_double_type_3_layer_sizes\n",
    "                ],\n",
    "                \"num_type2_layers\": CLA.srnet_double_num_type2_layers,\n",
    "                \"type_2_layer_feat_size\": CLA.srnet_double_type_2_layer_feat_size,\n",
    "                \"type_1_kernel_size\": CLA.srnet_double_type_1_kernel_size_spectral,\n",
    "            },\n",
    "            with_attention=CLA.srnet_double_with_attention,\n",
    "        )\n",
    "    else:\n",
    "        model_class = TimmModel\n",
    "        model_args = dict(model_name=model_name)\n",
    "\n",
    "    model_args[\"decision_threshold\"] = CLA.decision_threshold\n",
    "\n",
    "    if Paths.previous_checkpoint is not None and os.path.exists(\n",
    "        Paths.previous_checkpoint\n",
    "    ):\n",
    "        model = model_class.load_from_checkpoint(\n",
    "            Paths.previous_checkpoint, **model_args\n",
    "        )\n",
    "    else:\n",
    "        model = model_class(**model_args)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    if CLA.dataset == \"DFDC\":\n",
    "        if CLA.test_all_frames:\n",
    "            dataset = DFDC(\n",
    "                Paths.DFDC.test_set if CLA.test else Paths.DFDC.validation_set,\n",
    "                is_test=True,\n",
    "            )\n",
    "            return dataset\n",
    "        elif CLA.test or CLA.valid:\n",
    "            dataset = DFDC_preprocessed_single_frames(\n",
    "                (\n",
    "                    Paths.DFDC.preprocessed_dataset_single_frames_test\n",
    "                    if CLA.test\n",
    "                    else Paths.DFDC.preprocessed_dataset_single_frames_val\n",
    "                ),\n",
    "                transforms=get_transforms(\"test\"),\n",
    "            )\n",
    "            return dataset\n",
    "        else:  # training\n",
    "            train_set = DFDC_preprocessed_single_frames(\n",
    "                Paths.DFDC.preprocessed_dataset_single_frames_train,\n",
    "                transforms=get_transforms(\"train\"),\n",
    "            )\n",
    "\n",
    "            train_set = Oversampled(\n",
    "                train_set\n",
    "            )  # balance classes for training with oversampling\n",
    "\n",
    "            val_set = DFDC_preprocessed_single_frames(\n",
    "                Paths.DFDC.preprocessed_dataset_single_frames_val,\n",
    "                transforms=get_transforms(\"val\"),\n",
    "            )\n",
    "            return train_set, val_set\n",
    "    elif CLA.dataset == \"celebdfv2\":\n",
    "        cls, transforms, path = None, None, None\n",
    "        if CLA.test_all_frames:\n",
    "            cls = CelebDFV2\n",
    "            path = Paths.CelebDFV2.dataset_path\n",
    "        else:\n",
    "            cls = CelebDFV2_preprocessed\n",
    "            path = Paths.CelebDFV2.preprocessed_path\n",
    "            transforms = get_transforms(\"test\")\n",
    "\n",
    "        dataset = cls(path, is_train=not CLA.test, transforms=transforms)\n",
    "\n",
    "        if not CLA.test:\n",
    "            train_set, val_set = dataset.split_train_val(ratio=0.8)\n",
    "            # train_set, val_set = torch.utils.data.random_split(\n",
    "            #     dataset, (train_len, len(dataset) - train_len), generator=torch.Generator().manual_seed(CLA.seed)\n",
    "            # )\n",
    "            train_set.dataset = copy(dataset)  # Full copy for training transforms\n",
    "            train_set.dataset.transforms = get_transforms(\"train\")\n",
    "            if CLA.valid:\n",
    "                return val_set\n",
    "            else:\n",
    "                return Oversampled(train_set), val_set\n",
    "\n",
    "        return dataset\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555797e2-7d2d-4615-9b59-374935a04227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "del sys.argv[1:]\n",
    "\n",
    "\n",
    "\n",
    "sys.argv += [\n",
    "    \"--no_spectral_features\",\n",
    "    \"--model=srnet\",\n",
    "    \"--valid\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061b823-56e9-42e1-b87d-dafd11ee73df",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_args()\n",
    "CLA.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a20f34-5b9a-40bd-87db-bb8473fcb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paths.previous_checkpoint = \"./checkpoints/5090479/checkpoint_epoch=25-val_accuracy_epoch=0.76066-val_auroc_epoch=0.84424.ckpt\"\n",
    "Paths.previous_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415383de-f025-4cc0-813e-7144c75651c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = get_model()\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf135c7-856a-4aea-9083-65c40641667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "batch_size = 64\n",
    "data_loader_args = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"pin_memory\": True,\n",
    "}\n",
    "dataset = get_dataset()\n",
    "all_transforms = dataset.transforms\n",
    "dataset.transforms = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e902c-6438-46ed-92e0-f5d184cece11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Oversampled(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc682552-14af-4fed-ba3f-c35820bc0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.spatial_feat_extractor = model.ycc_feat_extractor\n",
    "# model.spectral_feat_extractor = model.dct_feat_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935f332-5d70-4f2f-ab12-552d5900af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.ycc_feat_extractor.feat_extractor[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c6c15-6cb7-49a2-9cfc-f525e64b5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(model.ycc_feat_extractor.feat_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a1f9b-231a-4b8b-818a-31465453c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    \"layer1\",\n",
    "    \"layer2\",\n",
    "    \"layer31\",\n",
    "    \"layer32\",\n",
    "    \"layer41\",\n",
    "    \"layer42\",\n",
    "    \"layer51\",\n",
    "    \"layer52\",\n",
    "    \"layer61\",\n",
    "    \"layer62\",\n",
    "    \"layer71\",\n",
    "    \"layer72\",\n",
    "    \"layer81\",\n",
    "    \"layer82\",\n",
    "    \"layer83\",\n",
    "    \"layer91\",\n",
    "    \"layer92\",\n",
    "    \"layer93\",\n",
    "    \"layer101\",\n",
    "    \"layer102\",\n",
    "    \"layer103\",\n",
    "    \"layer111\",\n",
    "    \"layer112\",\n",
    "    \"layer113\",\n",
    "    \"layer121\",\n",
    "    \"layer122\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75578dd8-d52f-4a91-b647-dfdea667c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_nb in range(len(layers)):\n",
    "    layer_name = layers[layer_nb]\n",
    "    layer = getattr(model.model, layer_name)\n",
    "    cls = gc.GradCAM\n",
    "    cam_ycc = cls(model=model, target_layers=[layer], use_cuda=True)\n",
    "    # cam_dct = cls(model=model, target_layers=[model.dct_feat_extractor.feat_extractor[layer_nb]], use_cuda=True)\n",
    "    cam_ycc.batch_size = 32\n",
    "    # cam_dct.batch_size=32\n",
    "\n",
    "    num_samples = 6\n",
    "    fig, axs = plt.subplots(num_samples, 2, figsize=(4 * 2, num_samples * 4))\n",
    "    fig.suptitle(f\"{cls.__name__} of feature extractors at layer {layer_nb}\")\n",
    "    np.random.seed(0x1B)\n",
    "\n",
    "    for i, rnd_sample in enumerate(np.random.choice(len(dataset), size=num_samples)):\n",
    "        img, label = dataset[rnd_sample]\n",
    "        img_tensor = all_transforms(img).unsqueeze(0)\n",
    "        # Real image after transforms\n",
    "\n",
    "        axs[i, 0].imshow(img)\n",
    "        axs[i, 0].set_title(\n",
    "            \"Image is \" + (\"real\" if dataset[rnd_sample][1] else \"fake\")\n",
    "        )\n",
    "\n",
    "        # Cam YCC\n",
    "        cam_ycc_out = cam_ycc(img_tensor, eigen_smooth=True)\n",
    "        axs[i, 1].imshow(cam_ycc_out[0])\n",
    "        axs[i, 1].set_title(f\"{cls.__name__} of image\")\n",
    "\n",
    "        # DCT Transform\n",
    "        # dct_tensor = model.post_dct(dct_2d(model.pre_dct(img_tensor)))\n",
    "        # dct_tensor = dct_tensor[0].permute(1, 2, 0).numpy()\n",
    "        # dct_tensor -= dct_tensor.min()\n",
    "        # dct_tensor /= dct_tensor.max()\n",
    "        # axs[i, 2].imshow(dct_tensor)\n",
    "\n",
    "        # axs[i, 2].set_title(f\"DCT of image\")\n",
    "\n",
    "        # Cam DCT\n",
    "        # cam_dct_out = cam_dct(img_tensor)\n",
    "        # axs[i, 3].imshow(cam_dct_out[0])\n",
    "        # axs[i, 3].set_title(f\"{cls.__name__} of DCT\")\n",
    "\n",
    "        # for j in range(4):\n",
    "        #    axs[i, j].set_axis_off()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\n",
    "        f\"grad_cam_dfdc_srnet/{cls.__name__}_layer_{layer_nb}.png\", bbox_inches=\"tight\"\n",
    "    )\n",
    "    # fig.savefig(f\"grad_cam_dfdc_srnet/{cls.__name__}_layer_{layer_nb}.eps\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45879f-135e-41ab-a986-40f98060f493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ec2b302a3623fafbdb4cbc01e9dd40c3cc226b6824da46d014e8d7143635447"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
